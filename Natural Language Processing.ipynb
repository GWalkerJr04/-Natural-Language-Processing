{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d3dba8-3a54-4ff8-a4e2-8d4845a81b03",
   "metadata": {},
   "source": [
    "#### Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c954dbf6-a025-40f0-bb66-9592b546a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f8c19-f840-44e1-b4d9-dcd6e2da9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]\n",
      "nltk: 3.8.1\n",
      "sklearn: 1.1.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('nltk: {}'.format(nltk.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd4101c-1817-4a85-b279-85c7ad93d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb297d-8b11-47fb-94dc-f810d366026b",
   "metadata": {},
   "source": [
    "## Corpus -\n",
    "Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.\n",
    "\n",
    "## Lexicon -\n",
    "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
    "\n",
    "## Token -\n",
    "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdad26fb-0ac4-4ce6-9bc9-71f492f1fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello students, how are you doing today?', 'The olympics are inspiring, and Python is awesome.', 'You look great today.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = 'Hello students, how are you doing today? The olympics are inspiring, and Python is awesome. You look great today.'\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2406c37a-6c8d-4826-a3e2-98fb3d2d3283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'students', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'olympics', 'are', 'inspiring', ',', 'and', 'Python', 'is', 'awesome', '.', 'You', 'look', 'great', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "220dfa82-0703-4b46-a363-1450aaabc27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'how', 'be', 'she', 'below', 'ain', 'off', 'these', 'o', 'than', 'has', 'then', 'have', 'but', 'nor', 'hers', 'the', 'll', 'very', 'other', 'are', 'why', 'its', 'few', 'while', 'mightn', \"shan't\", 'just', 'themselves', 'on', \"aren't\", 'myself', 'weren', 'who', 'doing', 'at', \"hadn't\", 'until', 'whom', \"that'll\", 'out', \"don't\", 'shan', 'did', 'won', 'above', 'those', 'were', \"doesn't\", 'to', 'y', 'doesn', 'when', 'which', 'here', 'into', 'from', 'where', 'most', 't', 'aren', \"couldn't\", 'if', 'is', 'theirs', 'he', \"should've\", 'under', 'a', 'each', 'yourself', 'more', 'not', 'your', 'any', 'itself', 'both', 'so', 've', 'didn', 'mustn', 'that', 'own', 'am', 'can', \"you're\", 'down', 'ours', 'haven', 'needn', 'with', 'him', \"it's\", 'was', 'don', 'i', 'wouldn', 'does', 'after', 'them', 'such', 'further', 'their', \"hasn't\", 'being', 'wasn', \"didn't\", 'should', 'my', 'his', 'over', \"shouldn't\", 'only', 'couldn', 'they', 'will', 'during', 'before', 'hasn', 'do', 'as', 'for', \"you'll\", \"wouldn't\", 'against', 'some', \"needn't\", \"wasn't\", 's', 'all', 'yours', 'in', 'once', 're', \"you've\", 'yourselves', 'had', \"haven't\", 'having', 'ourselves', 'by', 'now', 'what', 'between', 'isn', \"won't\", 'too', \"weren't\", \"you'd\", 'or', 'no', 'hadn', 'this', 'we', 'there', \"she's\", 'her', 'herself', 'because', 'himself', 'up', 'again', 'an', 'you', 'shouldn', 'been', 'of', \"isn't\", 'it', 'me', 'ma', 'about', 'our', 'through', \"mustn't\", 'm', 'd', 'and', \"mightn't\", 'same'}\n"
     ]
    }
   ],
   "source": [
    "# removing stop words - useless data\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319efc0b-8bc8-444a-8dae-04375520c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2092fc0c-fb81-407c-b484-31622dbdaead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n",
      "ride\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"ride\",\"riding\",\"rider\",\"rides\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c1724b6-0fb7-4526-b21c-b52c01c117b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cowboy\n",
      "rode\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming an entire sentence!\n",
    "\n",
    "new_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a1214-5504-4a58-8dc3-32d76d6036d6",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging with NLTK\r\n",
    "Part of speech tagging means labeling words as nouns, verbs, adjectives, etc. Even better, NLTK can handle tenses! While we're at it, we are also going to import a new sentence tokenizer (PunktSentenceTokenizer). This tokenizer is capable of unsupervised learning, so it can be trained on any body of text.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "497381e3-abc4-409c-92bc-b1355f679e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights\n",
      "Preamble\n",
      "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, \n",
      "\n",
      "Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people, \n",
      "\n",
      "Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law, \n",
      "\n",
      "Whereas it is essential to promote the development of friendly relations between nations, \n",
      "\n",
      "Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in the equal rights of men and women and have determined to promote social progress and better standards of life in larger freedom, \n",
      "\n",
      "Whereas Member States have pledged themselves to achieve, in cooperation with the United Nations, the promotion of universal respect for and observance of human rights and fundamental freedoms, \n",
      "\n",
      "Whereas a common understanding of these rights and freedoms is of the greatest importance for the full realization of this pledge, \n",
      "\n",
      "Now, therefore, \n",
      "\n",
      "The General Assembly, \n",
      "\n",
      "Proclaims this Universal Declaration of Human Rights as a common standard of achievement for all peoples and all nations, to the end that every individual and every organ of society, keeping this Declaration constantly in mind, shall strive by teaching and education to promote respect for these rights and freedoms and by progressive measures, national and international, to secure their universal and effective recognition and observance, both among the peoples of Member States themselves and among the peoples of territories under their jurisdiction. \n",
      "\n",
      "Article 1 \n",
      "All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. \n",
      "\n",
      "Article 2 \n",
      "Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. \n",
      "\n",
      "Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty. \n",
      "\n",
      "Article 3 \n",
      "Everyone has the right to life, liberty and security of person. \n",
      "\n",
      "Article 4 \n",
      "No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms. \n",
      "\n",
      "Article 5 \n",
      "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment. \n",
      "\n",
      "Article 6 \n",
      "Everyone has the right to recognition everywhere as a person before the law. \n",
      "\n",
      "Article 7 \n",
      "All are equal before the law and are entitled without any discrimination to equal protection of the law. All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination. \n",
      "\n",
      "Article 8 \n",
      "Everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law. \n",
      "\n",
      "Article 9 \n",
      "No one shall be subjected to arbitrary arrest, detention or exile. \n",
      "\n",
      "Article 10 \n",
      "Everyone is entitled in full equality to a fair and public hearing by an independent and impartial tribunal, in the determination of his rights and obligations and of any criminal charge against him. \n",
      "\n",
      "Article 11 \n",
      "Everyone charged with a penal offence has the right to be presumed innocent until proved guilty according to law in a public trial at which he has had all the guarantees necessary for his defence. \n",
      "No one shall be held guilty of any penal offence on account of any act or omission which did not constitute a penal offence, under national or international law, at the time when it was committed. Nor shall a heavier penalty be imposed than the one that was applicable at the time the penal offence was committed. \n",
      "Article 12 \n",
      "No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks. \n",
      "\n",
      "Article 13 \n",
      "Everyone has the right to freedom of movement and residence within the borders of each State. \n",
      "Everyone has the right to leave any country, including his own, and to return to his country. \n",
      "Article 14 \n",
      "Everyone has the right to seek and to enjoy in other countries asylum from persecution. \n",
      "This right may not be invoked in the case of prosecutions genuinely arising from non-political crimes or from acts contrary to the purposes and principles of the United Nations. \n",
      "Article 15 \n",
      "Everyone has the right to a nationality. \n",
      "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality. \n",
      "Article 16 \n",
      "Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. \n",
      "Marriage shall be entered into only with the free and full consent of the intending spouses. \n",
      "The family is the natural and fundamental group unit of society and is entitled to protection by society and the State. \n",
      "Article 17 \n",
      "Everyone has the right to own property alone as well as in association with others. \n",
      "No one shall be arbitrarily deprived of his property. \n",
      "Article 18 \n",
      "Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance. \n",
      "\n",
      "Article 19 \n",
      "Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. \n",
      "\n",
      "Article 20 \n",
      "Everyone has the right to freedom of peaceful assembly and association. \n",
      "No one may be compelled to belong to an association. \n",
      "Article 21 \n",
      "Everyone has the right to take part in the government of his country, directly or through freely chosen representatives. \n",
      "Everyone has the right to equal access to public service in his country. \n",
      "The will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures. \n",
      "Article 22 \n",
      "Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality. \n",
      "\n",
      "Article 23 \n",
      "Everyone has the right to work, to free choice of employment, to just and favourable conditions of work and to protection against unemployment. \n",
      "Everyone, without any discrimination, has the right to equal pay for equal work. \n",
      "Everyone who works has the right to just and favourable remuneration ensuring for himself and his family an existence worthy of human dignity, and supplemented, if necessary, by other means of social protection. \n",
      "Everyone has the right to form and to join trade unions for the protection of his interests. \n",
      "Article 24 \n",
      "Everyone has the right to rest and leisure, including reasonable limitation of working hours and periodic holidays with pay. \n",
      "\n",
      "Article 25 \n",
      "Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control. \n",
      "Motherhood and childhood are entitled to special care and assistance. All children, whether born in or out of wedlock, shall enjoy the same social protection. \n",
      "Article 26 \n",
      "Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit. \n",
      "Education shall be directed to the full development of the human personality and to the strengthening of respect for human rights and fundamental freedoms. It shall promote understanding, tolerance and friendship among all nations, racial or religious groups, and shall further the activities of the United Nations for the maintenance of peace. \n",
      "Parents have a prior right to choose the kind of education that shall be given to their children. \n",
      "Article 27 \n",
      "Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. \n",
      "Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. \n",
      "Article 28 \n",
      "Everyone is entitled to a social and international order in which the rights and freedoms set forth in this Declaration can be fully realized. \n",
      "\n",
      "Article 29 \n",
      "Everyone has duties to the community in which alone th\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b4c13a2-6e2f-4615-ba57-edb556756f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses. \n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c031bdb-2a6b-460e-a5ed-fd7f5938e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have some text, we can train the PunktSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc53649e-0a9f-4d7e-9cbf-ca84a560c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets tokenize the sample_text using our trained tokenizer\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d78055-8af5-4dce-9ad6-9c97b52e58d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# This function will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "# The output is a list of tuples - the word with it's part of speech\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634c12b-693a-434b-8c1b-458ae53bb3e0",
   "metadata": {},
   "source": [
    "## Chunking with NLTK\n",
    "\n",
    "Now that each word has been tagged with a part of speech, we can move onto chunking: grouping the words into meaningful clusters.  The main goal of chunking is to group words into \"noun phrases\", which is a noun with any associated verbs, adjectives, or adverbs. \n",
    "\n",
    "The part of speech tags that were generated in the previous step will be combined with regular expressions, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18ab685c-f80f-4fb0-b5fe-858dec6a4b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n+ = match 1 or more\\n? = match 0 or 1 repetitions.\\n* = match 0 or MORE repetitions\\t  \\n. = Any character except a new line\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ac1096-4780-4210-9eb9-e390fcd97ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # combine the part-of-speech tag with a regular expression\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # draw the chunks with nltk\n",
    "            # chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef27ade2-d20e-4bd3-9eb6-b36beb9dfb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "658c31a5-fe50-4494-8adb-ce3a3772fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \\n\\n<VB.?>* = \"0 or more of any tense of verb,\" followed by: \\n\\n<NNP>+ = \"One or more proper nouns,\" followed by \\n\\n<NN>? = \"zero or one singular noun.\" \\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \n",
    "\n",
    "<VB.?>* = \"0 or more of any tense of verb,\" followed by: \n",
    "\n",
    "<NNP>+ = \"One or more proper nouns,\" followed by \n",
    "\n",
    "<NN>? = \"zero or one singular noun.\" \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96ecc5-b191-4d4b-92a8-df56c95381c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the chunks, which are stored as an NLTK tree \n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # combine the part-of-speech tag with a regular expression\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            \n",
    "            # draw the chunks with nltk\n",
    "            # chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9473d39-a1df-4b1c-ab46-0945207f583b",
   "metadata": {},
   "source": [
    "## Chinking with NLTK\n",
    "\n",
    "Sometimes there are words in the chunks that we don't won't, we can remove them using a process called chinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11c5c8-13b2-4f7f-9ea4-7079e8af6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            # The main difference here is the }{, vs. the {}. This means we're removing \n",
    "            # from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            # print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            # chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56934b8-f58e-469c-900e-a1a6ad76fa67",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with NLTK\n",
    "\n",
    "One of the most common forms of chunking in natural language processing is called \"Named Entity Recognition.\" NLTK is able to identify people, places, things, locations, monetary figures, and more.\n",
    "\n",
    "There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.\n",
    "\n",
    "Here, with the option of binary = True, this means either something is a named entity, or not. There will be no further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8760656f-f912-46fd-9599-8e5a1b490842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            # namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1e7d6-f89a-41da-a7a9-f07ab69238bd",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "##### Text classification using NLTK\n",
    "\n",
    "Now that we have covered the basics of preprocessing for Natural Language Processing, we can move on to text classification using simple machine learning classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71c198b4-fd3a-4137-a621-3769d6207a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 2000\n",
      "First Review: (['delicatessen', '(', 'directors', ':', 'marc', 'caro', '/', 'jean', '-', 'pierre', 'jeunet', ';', 'screenwriters', ':', 'gilles', 'adrien', '/', 'marc', 'caro', ';', 'cinematographer', ':', 'darius', 'khondji', ';', 'editor', ':', 'herve', 'schneid', ';', 'cast', ':', 'dominique', 'pinon', '(', 'louison', ')', ',', 'marie', '-', 'laure', 'dougnac', '(', 'julie', 'clapet', ')', ',', 'jean', '-', 'claude', 'dreyfus', '(', 'clapet', '-', 'the', 'butcher', ')', ',', 'karin', 'viard', '(', 'mademoiselle', 'plusse', ')', ',', 'ticky', 'holgado', '(', 'marcel', 'tapioca', ')', ',', 'anne', '-', 'marie', 'pisani', '(', 'madame', 'tapioca', ')', ',', 'jacques', 'mathou', '(', 'roger', ')', ',', 'rufus', '(', 'robert', 'kube', ')', ',', 'howard', 'vernon', '(', 'frog', 'man', ')', ',', 'edith', 'ker', '(', 'granny', ')', ',', 'boban', 'janevski', '(', 'young', 'rascal', ')', ',', 'mikael', 'todde', '(', 'young', 'rascal', ')', ',', 'chick', 'ortega', '(', 'postman', ')', ',', 'silvie', 'laguna', '(', 'aurore', 'interligator', ')', ',', 'howard', 'vernon', '(', 'frog', 'man', ')', ';', 'runtime', ':', '96', ';', 'miramax', '/', 'constellation', '/', 'ugc', '/', 'hatchette', 'premiere', ';', '1991', '-', 'france', ')', 'reviewed', 'by', 'dennis', 'schwartz', 'a', 'black', 'comedy', 'set', 'in', 'the', 'near', 'future', 'in', 'a', 'boarding', 'house', 'run', 'by', 'a', 'depraved', 'butcher', '.', 'the', 'comedy', 'is', 'played', 'more', 'in', 'comic', 'strip', 'style', 'for', 'entertaining', 'value', 'than', 'for', 'deeper', 'satire', ',', 'as', 'it', 'features', 'mostly', 'zany', 'sophomoric', 'sight', 'gags', 'and', 'relies', 'heavily', 'on', 'special', 'effects', '.', 'the', 'world', 'has', 'fallen', 'on', 'hard', 'times', 'and', 'there', 'are', 'food', 'shortages', 'which', 'include', 'no', 'meat', ',', 'so', 'the', 'butcher', 'serves', 'up', 'meat', 'from', 'human', 'flesh', 'to', 'customers', 'who', 'pay', 'with', 'grain', ',', 'almost', 'as', 'valued', 'a', 'commodity', '.', 'that', \"'\", 's', 'the', 'big', 'joke', 'in', 'the', 'film', 'and', 'the', 'novelty', 'of', 'that', 'cannibalism', 'idea', 'wears', 'thin', 'mighty', 'fast', ',', 'as', 'the', 'characters', 'are', 'too', 'absurd', 'and', 'sketched', 'too', 'thinly', 'for', 'us', 'to', 'care', 'about', 'them', '.', 'this', 'tasteless', 'postapocalyptic', 'french', 'comedy', 'is', 'a', 'first', 'feature', 'for', 'the', 'co', '-', 'directors', 'marc', 'caro', '/', 'jean', '-', 'pierre', 'jeunet', '.', 'it', 'failed', 'to', 'reach', 'my', 'funny', 'bone', 'and', 'instead', 'left', 'me', 'mostly', 'annoyed', 'at', 'its', 'slight', 'story', 'and', 'its', 'dark', 'projections', 'for', 'the', 'future', '.', 'an', 'ex', '-', 'circus', 'clown', 'named', 'louison', '(', 'dominique', 'pinon', ')', ',', 'the', 'film', \"'\", 's', 'too', '-', 'good', '-', 'to', '-', 'be', '-', 'true', 'hero', ',', 'answers', 'an', 'ad', 'for', 'work', 'as', 'a', 'handyman', 'for', 'clapet', '(', 'jean', '-', 'claude', 'dreyfus', ')', ',', 'and', 'the', 'butcher', 'and', 'landlord', ',', 'offers', 'him', 'room', 'and', 'board', 'in', 'his', 'house', '.', 'the', 'butcher', \"'\", 's', 'clumsy', 'and', 'near', '-', 'sighted', 'daughter', 'julie', '(', 'marie', '-', 'laure', 'dougnac', ')', 'falls', 'in', 'love', 'with', 'the', 'skinny', ',', 'weird', 'looking', 'clown', ',', 'and', 'the', 'two', 'make', 'some', 'music', 'together', ',', 'with', 'her', 'playing', 'the', 'cello', 'and', 'him', 'a', 'saw', '.', 'they', 'are', 'the', 'innocents', ',', 'surrounded', 'by', 'a', 'boarding', 'house', 'of', 'misfits', 'suffering', 'from', 'fear', 'and', 'watched', 'over', 'by', 'her', 'overbearing', 'father', ',', 'who', 'has', 'lured', 'into', 'his', 'tenement', 'the', 'clown', ',', 'as', 'he', 'has', 'his', 'past', 'innocent', 'victims', ',', 'so', 'that', 'he', 'can', 'put', 'his', 'cleaver', 'to', 'him', 'and', 'then', 'sell', 'him', 'as', 'meat', ',', 'which', 'he', 'intends', 'to', 'do', 'as', 'soon', 'as', 'the', 'clown', 'fixes', 'up', 'the', 'tenement', '.', 'the', 'entire', 'film', 'takes', 'place', 'in', 'the', 'shabby', 'tenement', ',', 'and', 'the', 'tenants', 'are', 'an', 'odd', 'lot', 'of', 'bizarre', 'malcontents', ',', 'who', 'do', 'not', 'trust', 'each', 'other', '.', 'there', 'are', 'two', 'youngsters', '(', 'boban', 'janevski', '&', 'mikael', 'todde', ')', 'who', 'do', 'any', 'kind', 'of', 'mischief', 'they', 'can', '.', 'a', 'frog', 'man', '(', 'howard', 'vernon', ')', 'who', 'lives', 'with', 'water', 'on', 'the', 'floor', 'so', 'he', 'can', 'raise', 'his', 'frogs', 'and', 'snails', 'that', 'he', 'eats', '.', 'two', 'brothers', '(', 'mathou', '&', 'kube', ')', 'who', 'create', 'little', 'cow', '-', 'moo', 'novelty', 'toys', '.', 'a', 'man', '(', 'holgado', ')', 'who', 'sells', 'a', 'bullshit', 'detector', 'to', 'the', 'butcher', 'for', 'his', 'piece', 'of', 'meat', '.', 'a', 'slutty', 'woman', '(', 'karin', 'viard', ')', ',', 'who', 'lives', 'with', 'the', 'butcher', 'and', 'only', 'wants', 'his', 'meat', '.', 'the', 'aristocratic', 'woman', '(', 'silvie', 'laguna', ')', 'who', 'tries', 'numerous', 'times', 'to', 'commit', 'suicide', 'but', 'is', 'too', 'inept', 'to', 'do', 'it', 'right', '.', 'the', 'tenants', 'are', 'too', 'afraid', 'to', 'come', 'out', 'at', 'night', 'because', 'they', 'know', 'what', 'the', 'butcher', 'is', 'up', 'to', ',', 'so', 'they', 'are', 'forced', 'to', 'communicate', 'with', 'each', 'other', 'through', 'a', 'pipe', 'that', 'runs', 'through', 'the', 'building', '(', 'in', 'one', 'scene', 'they', 'are', 'all', 'in', 'musical', 'harmony', 'to', 'the', 'lovemaking', 'of', 'the', 'butcher', 'and', 'his', 'gal', ',', 'as', 'their', 'bedsprings', 'squeak', ')', '.', 'there', 'is', 'also', 'a', 'sex', '-', 'crazed', 'postman', '(', 'chick', 'ortega', ')', ',', 'who', 'lusts', 'for', 'the', 'butcher', \"'\", 's', 'daughter', 'and', 'carries', 'a', 'gun', 'while', 'delivering', 'the', 'mail', '.', 'there', 'is', 'also', 'an', 'underdeveloped', 'subplot', 'about', 'a', 'band', 'of', 'incompetent', 'underground', 'veggie', 'fanatics', ',', 'called', 'trogolodistes', ',', 'who', 'have', 'been', 'summoned', 'to', 'rescue', 'the', 'clown', 'and', 'steal', 'some', 'grain', '.', 'the', 'directors', 'overloaded', 'the', 'film', 'with', 'too', 'many', 'eccentrics', ',', 'as', 'the', 'comedy', 'seemed', 'forced', 'while', 'the', 'surreal', 'look', 'of', 'the', 'film', 'added', 'no', 'dramatic', 'intensity', '.', 'delicatessen', 'could', 'have', 'some', 'appeal', 'to', 'the', 'cult', 'film', 'crowd', 'who', 'like', 'their', 'meat', 'sliced', 'thin', ',', 'monty', 'python', 'fans', ',', 'and', 'those', 'who', 'liked', 'terry', 'gilliam', \"'\", 's', 'brazil', ',', 'a', 'film', 'similar', 'in', 'spirit', '.'], 'neg')\n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {}'.format(len(documents)))\n",
    "print('First Review: {}'.format(documents[1]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))\n",
    "print('The word happy: {}'.format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c21916-9088-4719-adf3-1c1e41fbc7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "# We'll use the 4000 most common words as features\n",
    "print(len(all_words))\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef635e1a-2c3b-41a7-bccc-8d9547ec0491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      ":\n",
      "two\n",
      "teen\n",
      "couples\n",
      "go\n",
      "to\n",
      "a\n",
      "church\n",
      "party\n",
      ",\n",
      "drink\n",
      "and\n",
      "then\n",
      "drive\n",
      ".\n",
      "they\n",
      "get\n",
      "into\n",
      "an\n",
      "accident\n",
      "one\n",
      "of\n",
      "the\n",
      "guys\n",
      "dies\n",
      "but\n",
      "his\n",
      "girlfriend\n",
      "continues\n",
      "see\n",
      "him\n",
      "in\n",
      "her\n",
      "life\n",
      "has\n",
      "nightmares\n",
      "what\n",
      "'\n",
      "s\n",
      "deal\n",
      "?\n",
      "watch\n",
      "movie\n",
      "\"\n",
      "sorta\n",
      "find\n",
      "out\n",
      "critique\n",
      "mind\n",
      "-\n",
      "fuck\n",
      "for\n",
      "generation\n",
      "that\n",
      "touches\n",
      "on\n",
      "very\n",
      "cool\n",
      "idea\n",
      "presents\n",
      "it\n",
      "bad\n",
      "package\n",
      "which\n",
      "is\n",
      "makes\n",
      "this\n",
      "review\n",
      "even\n",
      "harder\n",
      "write\n",
      "since\n",
      "i\n",
      "generally\n",
      "applaud\n",
      "films\n",
      "attempt\n",
      "break\n",
      "mold\n",
      "mess\n",
      "with\n",
      "your\n",
      "head\n",
      "such\n",
      "(\n",
      "lost\n",
      "highway\n",
      "&\n",
      "memento\n",
      ")\n",
      "there\n",
      "are\n",
      "good\n",
      "ways\n",
      "making\n",
      "all\n",
      "types\n",
      "these\n",
      "folks\n",
      "just\n",
      "didn\n",
      "t\n",
      "snag\n",
      "correctly\n",
      "seem\n",
      "have\n",
      "taken\n",
      "pretty\n",
      "neat\n",
      "concept\n",
      "executed\n",
      "terribly\n",
      "so\n",
      "problems\n",
      "well\n",
      "its\n",
      "main\n",
      "problem\n",
      "simply\n",
      "too\n",
      "jumbled\n",
      "starts\n",
      "off\n",
      "normal\n",
      "downshifts\n",
      "fantasy\n",
      "world\n",
      "you\n",
      "as\n",
      "audience\n",
      "member\n",
      "no\n",
      "going\n",
      "dreams\n",
      "characters\n",
      "coming\n",
      "back\n",
      "from\n",
      "dead\n",
      "others\n",
      "who\n",
      "look\n",
      "like\n",
      "strange\n",
      "apparitions\n",
      "disappearances\n",
      "looooot\n",
      "chase\n",
      "scenes\n",
      "tons\n",
      "weird\n",
      "things\n",
      "happen\n",
      "most\n",
      "not\n",
      "explained\n",
      "now\n",
      "personally\n",
      "don\n",
      "trying\n",
      "unravel\n",
      "film\n",
      "every\n",
      "when\n",
      "does\n",
      "give\n",
      "me\n",
      "same\n",
      "clue\n",
      "over\n",
      "again\n",
      "kind\n",
      "fed\n",
      "up\n",
      "after\n",
      "while\n",
      "biggest\n",
      "obviously\n",
      "got\n",
      "big\n",
      "secret\n",
      "hide\n",
      "seems\n",
      "want\n",
      "completely\n",
      "until\n",
      "final\n",
      "five\n",
      "minutes\n",
      "do\n",
      "make\n",
      "entertaining\n",
      "thrilling\n",
      "or\n",
      "engaging\n",
      "meantime\n",
      "really\n",
      "sad\n",
      "part\n",
      "arrow\n",
      "both\n",
      "dig\n",
      "flicks\n",
      "we\n",
      "actually\n",
      "figured\n",
      "by\n",
      "half\n",
      "way\n",
      "point\n",
      "strangeness\n",
      "did\n",
      "start\n",
      "little\n",
      "bit\n",
      "sense\n",
      "still\n",
      "more\n",
      "guess\n",
      "bottom\n",
      "line\n",
      "movies\n",
      "should\n",
      "always\n",
      "sure\n",
      "before\n",
      "given\n",
      "password\n",
      "enter\n",
      "understanding\n",
      "mean\n",
      "showing\n",
      "melissa\n",
      "sagemiller\n",
      "running\n",
      "away\n",
      "visions\n",
      "about\n",
      "20\n",
      "throughout\n",
      "plain\n",
      "lazy\n",
      "!\n",
      "okay\n",
      "people\n",
      "chasing\n",
      "know\n",
      "need\n",
      "how\n",
      "giving\n",
      "us\n",
      "different\n",
      "offering\n",
      "further\n",
      "insight\n",
      "down\n",
      "apparently\n",
      "studio\n",
      "took\n",
      "director\n",
      "chopped\n",
      "themselves\n",
      "shows\n",
      "might\n",
      "ve\n",
      "been\n",
      "decent\n",
      "here\n",
      "somewhere\n",
      "suits\n",
      "decided\n",
      "turning\n",
      "music\n",
      "video\n",
      "edge\n",
      "would\n",
      "actors\n",
      "although\n",
      "wes\n",
      "bentley\n",
      "seemed\n",
      "be\n",
      "playing\n",
      "exact\n",
      "character\n",
      "he\n",
      "american\n",
      "beauty\n",
      "only\n",
      "new\n",
      "neighborhood\n",
      "my\n",
      "kudos\n",
      "holds\n",
      "own\n",
      "entire\n",
      "feeling\n",
      "unraveling\n",
      "overall\n",
      "doesn\n",
      "stick\n",
      "because\n",
      "entertain\n",
      "confusing\n",
      "rarely\n",
      "excites\n",
      "feels\n",
      "redundant\n",
      "runtime\n",
      "despite\n",
      "ending\n",
      "explanation\n",
      "craziness\n",
      "came\n",
      "oh\n",
      "horror\n",
      "slasher\n",
      "flick\n",
      "packaged\n",
      "someone\n",
      "assuming\n",
      "genre\n",
      "hot\n",
      "kids\n",
      "also\n",
      "wrapped\n",
      "production\n",
      "years\n",
      "ago\n",
      "sitting\n",
      "shelves\n",
      "ever\n",
      "whatever\n",
      "skip\n",
      "where\n",
      "joblo\n",
      "nightmare\n",
      "elm\n",
      "street\n",
      "3\n",
      "7\n",
      "/\n",
      "10\n",
      "blair\n",
      "witch\n",
      "2\n",
      "crow\n",
      "9\n",
      "salvation\n",
      "4\n",
      "stir\n",
      "echoes\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# The find_features function will determine which of the 3000 word features are contained in the review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# Lets use an example from a negative review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ef77197-02b7-417e-b0a6-cd24f7914466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b0c166-57da-4479-a7ed-deaa1fde8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f2f9136-897c-44a5-b501-89078cb90602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cddab4e6-bf3d-4f6e-9791-6951b0fd9bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 81.6\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cffa93-bb80-47c2-b52e-baa68fe367ff",
   "metadata": {},
   "source": [
    "In this project, I built a foundation for Natural Language Processing in Python. I covered tokenizing, stemming, part of speech tagging, chunking, named entity recognition, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4be0f7-ba4f-43fe-ad85-2a523ce0f7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
